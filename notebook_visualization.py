# -*- coding: utf-8 -*-
"""notebook_visualization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vgTBpbv5Tp0AS1e4E_lmj-kKub8Ac1pE
"""

# Install required packages.
import os
import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git

import torch
from torch_geometric.datasets import TUDataset

dataset = TUDataset(root='data/TUDataset', name='MUTAG')
#PTC_MR
#NCI1

print()
print(f'Dataset: {dataset}:')
print('====================')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')

data = dataset[0]  # Get the first graph object.
num_features = dataset.num_features
print()
print(data)
print('=============================================================')

# Gather some statistics about the first graph.
print(f'Number of nodes: {data.num_nodes}')
print(f'Number of edges: {data.num_edges}')
print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')
print(f'Has isolated nodes: {data.has_isolated_nodes()}')
print(f'Has self-loops: {data.has_self_loops()}')
print(f'Is undirected: {data.is_undirected()}')

"""# the node fetaures are
Carbon (C)
Nitrogen (N)
Oxygen (O)
Fluorine (F)
Iodine (I)
Chlorine (Cl)
Bromine (Br)

The 2 classes are Mutageniuc an dnit mutagenic
"""















import random
import numpy as np
import torch

def seed_everything(seed=1234):
    """
    Seed everything to make all operations deterministic.
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)  # For CUDA operations, if applicable
    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups, if applicable
    torch.backends.cudnn.deterministic = True  # For deterministic algorithm selection in CuDNN
    torch.backends.cudnn.benchmark = False

# Now you can call the function
seed_everything(1234)

import torch
from torch_geometric.data import Data
from torch_geometric.utils import to_dense_adj, dense_to_sparse
import numpy as np
from scipy.linalg import expm


seed_everything(1234)

new_data = []
for d in dataset:
    # Convert PyG adjacency matrix to dense format
    adj_matrix = to_dense_adj(d.edge_index)[0].numpy()

    # Calculate the exponential of the adjacency matrix
    exp_adj_matrix = expm(adj_matrix)
    np.fill_diagonal(exp_adj_matrix,0)

    A = torch.tensor(exp_adj_matrix, dtype=torch.int)


    edge_index = (A > 0).nonzero().t().to(torch.long)

    edge_weights = A[edge_index[0], edge_index[1]].to(torch.float)
    new_d = Data(x=d.x, edge_index=edge_index, edge_weights=edge_weights, y=d.y)
    new_data.append(new_d)

from torch_geometric.utils import to_networkx
import matplotlib.pyplot as plt
import networkx as nx

#plot the first graph
d = new_data[0]
g = nx.Graph()
g.add_nodes_from(range(d.num_nodes))
g.add_edges_from(d.edge_index.T.tolist())
print(g.edges())
pos = nx.spring_layout(g)

nx.draw(g, pos, with_labels=True, node_size=500, node_color="skyblue", font_size=10, font_color="black")

edge_labels = {tuple(e): f"{int(d.edge_weights[i])}" for i, e in enumerate(d.edge_index.T.tolist())}
nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels)

plt.show()

g = to_networkx(data, to_undirected= True)
pos = nx.spring_layout(g)
print(g.edges())
nx.draw(g, pos, with_labels=True, node_size=500, node_color="skyblue", font_size=10, font_color="black")

# edge_labels = {tuple(e): f"{int(d.edge_weights[i])}" for i, e in enumerate(d.edge_index.T.tolist())}
# nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels)

plt.show()

for i,d in enumerate(new_data):
  print(d.edge_weights)
  if i==3:
    break

edge_index

import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split
from torch_geometric.data import DataLoader
import torch

# Assuming `new_data` is your dataset and you have extracted `labels`
labels = np.array([data.y.item() for data in new_data])

# Split the dataset into a training+validation set and a separate test set
train_val_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.2, stratify=labels, random_state=12345,shuffle= True)

train_val_data = [new_data[i] for i in train_val_idx]
test_data = [new_data[i] for i in test_idx]
test_labels = labels[test_idx]

# Initialize StratifiedKFold for cross-validation on the training+validation set
# Extract labels for the training+validation set for stratified cross-validation
train_val_labels = labels[train_val_idx]
    # Insert your model training and validation code here
# After cross-validation, you can evaluate your final model on the test set
test_loader = DataLoader(test_data, batch_size=16, shuffle=False)
# Perform final evaluation on test_loader

import torch
import math
from torch.nn import ModuleList
from torch_geometric.nn import MLP, global_mean_pool

class GCN_weighted(torch.nn.Module):
    def __init__(self, train_data, num_features, hidden_channels, num_layers, gnn):
        super().__init__()
        num_nodes = sorted([data.num_nodes for data in train_data])
        k = num_nodes[int(math.ceil(0.6 * len(num_nodes))) - 1]
        self.k = max(10, k)
        GNN = eval(gnn)  # dynamically creates a GNN class

        self.convs = ModuleList()
        self.convs.append(GNN(num_features, hidden_channels))
        for i in range(num_layers - 1):
            self.convs.append(GNN(hidden_channels, hidden_channels))
        # No need to add an extra GNN layer here as it's already added in the loop

        # Adjust the input size of MLP to match the concatenated features
        mlp_input_size = hidden_channels * num_layers
        self.mlp = MLP([mlp_input_size, 32, 1], dropout=0.5, batch_norm=False)

    def forward(self, x, edge_index, edge_weights, batch):
        xs = [x]
        for conv in self.convs:
            xs.append(torch.tanh(conv(xs[-1], edge_index, edge_weights)))
        x = torch.cat(xs[1:], dim=-1)  # Concatenate along the feature dimension

        x = global_mean_pool(x, batch)
        # No need to print the shape here in the final model
        return self.mlp(x)

import torch
from torch.nn import ModuleList
from torch_geometric.nn import MLP, global_mean_pool, GraphConv, SAGEConv, GCNConv, ResGatedGraphConv, TransformerConv, GATConv
import numpy as np
from scipy.linalg import expm
from torch_geometric.data import Data, DataLoader
from sklearn.model_selection import StratifiedKFold
import torch
from torch_geometric.data import Data
from torch_geometric.utils import to_dense_adj, dense_to_sparse
import numpy as np
from scipy.linalg import expm
from torch_geometric import seed_everything

seed_everything(1234)
from torch_geometric.loader import DataLoader
from torch.nn import BCEWithLogitsLoss
criterion = torch.nn.BCEWithLogitsLoss()

# Define the function to set random seed

# Set random seed
seed_everything(1234)

def train_weights(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0
    for data in loader:
        optimizer.zero_grad()
        # Ensure data is passed correctly
        out = model(data.x, data.edge_index, data.edge_weights, data.batch)
        out = torch.squeeze(out)
        target = data.y.type_as(out)  # Convert target to same data type as model output
        loss = criterion(out, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def test_weights(model, loader):
    model.eval()
    correct = 0
    for data in loader:
        out = model(data.x, data.edge_index, data.edge_weights, data.batch)
        out = torch.squeeze(out)
        pred = out.argmax(dim=1) if out.dim() > 1 else (out > 0).long()
        correct += int((pred == data.y).sum())
    return correct / len(loader.dataset)



# Set up Cross-Validation
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=12345)

# Initialize lists to store test accuracy
test_accuracy = []
# Loop over folds
for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(train_val_labels)), train_val_labels)):
    train_subset = [train_val_data[i] for i in train_idx]
    val_subset = [train_val_data[i] for i in val_idx]

    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=8, shuffle=False)

    model = GCN_weighted(hidden_channels=32,train_data = train_loader, num_features=num_features, num_layers=3, gnn= "GraphConv")
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    print(f'Fold {fold+1}:')
    print(f'Number of training graphs: {len(train_subset)}')
    print(f'Number of validation graphs: {len(val_subset)}')

    best_val_acc = 0
    for epoch in range(1, 100):
        loss = train_weights(model = model, loader = train_loader, optimizer = optimizer, criterion=criterion)
        train_acc = test_weights(model, train_loader)
        val_acc = test_weights(model, val_loader)
        test_acc = test_weights(model, test_loader)

        if epoch%10==0:
          print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, val Acc: {val_acc:.4f}  test_acc: {test_acc}')
        if val_acc>best_val_acc:
          best_val_acc = val_acc
          torch.save(model.state_dict(),'model_with_weights-best-acc.pkl')
          #### save your model in a file : model.save_to_dict

    model.load_state_dict(torch.load('model_with_weights-best-acc.pkl'))
    model.eval()
    test_acc_ = test_weights(model,test_loader)
    test_accuracy.append(test_acc_)


print("final mean accuracy of this model: ", np.mean(test_accuracy))

import torch
from torch.nn import ModuleList
from torch_geometric.nn import MLP, global_mean_pool, GraphConv, SAGEConv, GCNConv, ResGatedGraphConv, TransformerConv, GATConv
import numpy as np
from scipy.linalg import expm
from torch_geometric.data import Data, DataLoader
from sklearn.model_selection import StratifiedKFold
import torch
from torch_geometric.data import Data
from torch_geometric.utils import to_dense_adj, dense_to_sparse
import numpy as np
from scipy.linalg import expm
from torch_geometric import seed_everything

seed_everything(1234)
from torch_geometric.loader import DataLoader
from torch.nn import BCEWithLogitsLoss
criterion = torch.nn.BCEWithLogitsLoss()

# Define the function to set random seed

# Set random seed
seed_everything(1234)

def train_weights(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0
    for data in loader:
        optimizer.zero_grad()
        # Ensure data is passed correctly
        out = model(data.x, data.edge_index, None, data.batch)
        out = torch.squeeze(out)
        target = data.y.type_as(out)  # Convert target to same data type as model output
        loss = criterion(out, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def test_weights(model, loader):
    model.eval()
    correct = 0
    for data in loader:
        out = model(data.x, data.edge_index, None, data.batch)
        out = torch.squeeze(out)
        pred = out.argmax(dim=1) if out.dim() > 1 else (out > 0).long()
        correct += int((pred == data.y).sum())
    return correct / len(loader.dataset)



# Set up Cross-Validation
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=12345)

# Initialize lists to store test accuracy
test_accuracy = []

# Loop over folds
for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(train_val_labels)), train_val_labels)):
    train_subset = [train_val_data[i] for i in train_idx]
    val_subset = [train_val_data[i] for i in val_idx]

    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=8, shuffle=False)

    model = GCN_weighted(hidden_channels=32,train_data = train_loader, num_features=num_features, num_layers=3, gnn= "GraphConv")
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    print(f'Fold {fold+1}:')
    print(f'Number of training graphs: {len(train_subset)}')
    print(f'Number of validation graphs: {len(val_subset)}')

    best_val_acc = 0
    for epoch in range(1, 100):
        loss = train_weights(model = model, loader = train_loader, optimizer = optimizer, criterion=criterion)
        train_acc = test_weights(model, train_loader)
        val_acc = test_weights(model, val_loader)
        test_acc = test_weights(model, test_loader)

        if epoch%10==0:
          print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, val Acc: {val_acc:.4f}  test_acc: {test_acc}')
        if val_acc>best_val_acc:
          best_val_acc = val_acc
          torch.save(model.state_dict(),'model_with_weights-best-acc.pkl')
          #### save your model in a file : model.save_to_dict

    model.load_state_dict(torch.load('model_with_weights-best-acc.pkl'))
    model.eval()
    test_acc_ = test_weights(model,test_loader)
    test_accuracy.append(test_acc_)


print("final mean accuracy of this model: ", np.mean(test_accuracy))

















