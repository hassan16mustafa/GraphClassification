# -*- coding: utf-8 -*-
"""graphclassificationrevised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/hassan16mustafa/b55ab1d49d081860f94a73a89979d4dc/graphclassificationrevised.ipynb
"""

# Install required packages.
import os
import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git

import torch
from torch_geometric.data import Data
from torch_geometric.utils import to_dense_adj, dense_to_sparse
import numpy as np
from scipy.linalg import expm
import random
from torch_geometric.datasets import TUDataset
from sklearn.model_selection import StratifiedKFold, train_test_split
from torch_geometric.data import DataLoader
import math
from torch.nn import ModuleList
from torch_geometric.nn import MLP, global_mean_pool
from torch.nn import ModuleList
from torch_geometric.nn import MLP, global_mean_pool, GraphConv, SAGEConv, GCNConv, ResGatedGraphConv, TransformerConv, GATConv

from scipy.linalg import expm
from torch_geometric.data import Data, DataLoader
from sklearn.model_selection import StratifiedKFold
from torch_geometric.data import Data
from torch_geometric.utils import to_dense_adj, dense_to_sparse
from scipy.linalg import expm
import csv
from torch_geometric.loader import DataLoader
from torch.nn import BCEWithLogitsLoss
criterion = torch.nn.BCEWithLogitsLoss()

def seed_everything(seed=1234):
    """
    Seed everything to make all operations deterministic.
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)  # For CUDA operations, if applicable
    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups, if applicable
    torch.backends.cudnn.deterministic = True  # For deterministic algorithm selection in CuDNN
    torch.backends.cudnn.benchmark = False

# Now you can call the function
seed_everything(1234)

class GCN_weighted(torch.nn.Module):
    def __init__(self, train_data, num_features, hidden_channels, num_layers, gnn):
        super().__init__()
        num_nodes = sorted([data.num_nodes for data in train_data])
        k = num_nodes[int(math.ceil(0.6 * len(num_nodes))) - 1]
        self.k = max(10, k)
        GNN = eval(gnn)  # dynamically creates a GNN class

        self.convs = ModuleList()
        self.convs.append(GNN(num_features, hidden_channels))
        for i in range(num_layers - 1):
            self.convs.append(GNN(hidden_channels, hidden_channels))
        # No need to add an extra GNN layer here as it's already added in the loop

        # Adjust the input size of MLP to match the concatenated features
        mlp_input_size = hidden_channels * num_layers
        self.mlp = MLP([mlp_input_size, 32, 1], dropout=0.5, batch_norm=False)

    def forward(self, x, edge_index, edge_weights, batch):
        xs = [x]
        for conv in self.convs:
            xs.append(torch.tanh(conv(xs[-1], edge_index, edge_weights)))
        x = torch.cat(xs[1:], dim=-1)  # Concatenate along the feature dimension

        x = global_mean_pool(x, batch)
        # No need to print the shape here in the final model
        return self.mlp(x)

def train_weights(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0
    for data in loader:
        optimizer.zero_grad()
        # Ensure data is passed correctly
        out = model(data.x, data.edge_index, data.edge_weights, data.batch)
        out = torch.squeeze(out)
        target = data.y.type_as(out)  # Convert target to same data type as model output
        loss = criterion(out, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def test_weights(model, loader):
    model.eval()
    correct = 0
    for data in loader:
        out = model(data.x, data.edge_index, data.edge_weights, data.batch)
        out = torch.squeeze(out)
        pred = out.argmax(dim=1) if out.dim() > 1 else (out > 0).long()
        correct += int((pred == data.y).sum())
    return correct / len(loader.dataset)

### running the model with communicability
datasets = ["MUTAG"] # other datasets such as NCI1 can bee added to this list accordingly
file = open("results.csv", mode='a', newline='')
writer = csv.writer(file)
# Write header
writer.writerow(["Dataset", "communicability","Batch Size", "Learning Rate","Num Folds", "Val Acc", "Test Acc"])
batch_size = 16
lr = 0.01
num_folds = 10
for name in datasets:
  dataset = TUDataset(root='data/TUDataset', name=name)
  print()
  print(f'Dataset: {dataset}:')
  print('====================')
  print(f'Number of graphs: {len(dataset)}')
  print(f'Number of features: {dataset.num_features}')
  print(f'Number of classes: {dataset.num_classes}')

  data = dataset[0]  # Get the first graph object.
  num_features = dataset.num_features
  seed_everything(1234)

  new_data = []
  for d in dataset:
      # Convert PyG adjacency matrix to dense format
      adj_matrix = to_dense_adj(d.edge_index)[0].numpy()

      # Calculate the exponential of the adjacency matrix
      exp_adj_matrix = expm(adj_matrix)
      np.fill_diagonal(exp_adj_matrix,0)

      A = torch.tensor(exp_adj_matrix, dtype=torch.int)


      edge_index = (A > 0).nonzero().t().to(torch.long)

      edge_weights = A[edge_index[0], edge_index[1]].to(torch.float)
      new_d = Data(x=d.x, edge_index=edge_index, edge_weights=edge_weights, y=d.y)
      new_data.append(new_d)

  labels = np.array([data.y.item() for data in new_data])
  train_val_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.2, stratify=labels, random_state=12345,shuffle= True)

  train_val_data = [new_data[i] for i in train_val_idx]
  test_data = [new_data[i] for i in test_idx]
  test_labels = labels[test_idx]

  # Initialize StratifiedKFold for cross-validation on the training+validation set
  # Extract labels for the training+validation set for stratified cross-validation
  train_val_labels = labels[train_val_idx]
  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
  criterion = torch.nn.BCEWithLogitsLoss()
  skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=12345)

  # Initialize lists to store test accuracy
  test_accuracy,val_accuracy = [],[]
  # Loop over folds
  for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(train_val_labels)), train_val_labels)):
      train_subset = [train_val_data[i] for i in train_idx]
      val_subset = [train_val_data[i] for i in val_idx]

      train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
      val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

      model = GCN_weighted(hidden_channels=32,train_data = train_loader, num_features=num_features, num_layers=3, gnn= "GraphConv")
      optimizer = torch.optim.Adam(model.parameters(), lr=lr)

      print(f'Fold {fold+1}:')
      print(f'Number of training graphs: {len(train_subset)}')
      print(f'Number of validation graphs: {len(val_subset)}')

      best_val_acc = 0
      for epoch in range(1, 100):
          loss = train_weights(model = model, loader = train_loader, optimizer = optimizer, criterion=criterion)
          train_acc = test_weights(model, train_loader)
          val_acc = test_weights(model, val_loader)
          test_acc = test_weights(model, test_loader)

          if epoch%30==0:
            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, val Acc: {val_acc:.4f}  test_acc: {test_acc}')
          if val_acc>best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(),'model_with_weights-best-acc.pkl')
            #### save your model in a file : model.save_to_dict

      model.load_state_dict(torch.load('model_with_weights-best-acc.pkl'))
      model.eval()
      test_acc_ = test_weights(model,test_loader)
      test_accuracy.append(test_acc_)
      val_accuracy.append(best_val_acc)


  res = [name, "yes",batch_size, lr, num_folds, np.mean(val_acc),np.mean(test_accuracy)]
  print(res)
  writer.writerow(res)
file.close()
print("experiment completed")

class GCN_weighted(torch.nn.Module):
    def __init__(self, train_data, num_features, hidden_channels, num_layers, gnn):
        super().__init__()
        num_nodes = sorted([data.num_nodes for data in train_data])
        k = num_nodes[int(math.ceil(0.6 * len(num_nodes))) - 1]
        self.k = max(10, k)
        GNN = eval(gnn)  # dynamically creates a GNN class

        self.convs = ModuleList()
        self.convs.append(GNN(num_features, hidden_channels))
        for i in range(num_layers - 1):
            self.convs.append(GNN(hidden_channels, hidden_channels))
        # No need to add an extra GNN layer here as it's already added in the loop

        # Adjust the input size of MLP to match the concatenated features
        mlp_input_size = hidden_channels * num_layers
        self.mlp = MLP([mlp_input_size, 32, 1], dropout=0.5, batch_norm=False)

    def forward(self, x, edge_index, edge_weights, batch):
        xs = [x]
        for conv in self.convs:
            xs.append(torch.tanh(conv(xs[-1], edge_index, edge_weights)))
        x = torch.cat(xs[1:], dim=-1)  # Concatenate along the feature dimension

        x = global_mean_pool(x, batch)
        # No need to print the shape here in the final model
        return self.mlp(x)

def train_weights(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0
    for data in loader:
        optimizer.zero_grad()
        # Ensure data is passed correctly
        out = model(data.x, data.edge_index, None, data.batch)
        out = torch.squeeze(out)
        target = data.y.type_as(out)  # Convert target to same data type as model output
        loss = criterion(out, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def test_weights(model, loader):
    model.eval()
    correct = 0
    for data in loader:
        out = model(data.x, data.edge_index, None, data.batch)
        out = torch.squeeze(out)
        pred = out.argmax(dim=1) if out.dim() > 1 else (out > 0).long()
        correct += int((pred == data.y).sum())
    return correct / len(loader.dataset)



seed_everything(1234)
### running the model without communicability
datasets = ["MUTAG","PTC_MR"] # other datasets such as NCI1 can bee added to this list accordingly
file = open("results.csv", mode='a', newline='')
writer = csv.writer(file)
# Write header
# writer.writerow(["Dataset", "communicability","Batch Size", "Learning Rate","Num Folds", "Val Acc", "Test Acc"])
batch_size = 16
lr = 0.01
num_folds = 10
for name in datasets:
  dataset = TUDataset(root='data/TUDataset', name=name)
  print()
  print(f'Dataset: {dataset}:')
  print('====================')
  print(f'Number of graphs: {len(dataset)}')
  print(f'Number of features: {dataset.num_features}')
  print(f'Number of classes: {dataset.num_classes}')

  data = dataset[0]  # Get the first graph object.
  num_features = dataset.num_features
  seed_everything(1234)

  new_data = []
  for d in dataset:
      # Convert PyG adjacency matrix to dense format
      adj_matrix = to_dense_adj(d.edge_index)[0].numpy()

      # Calculate the exponential of the adjacency matrix
      exp_adj_matrix = expm(adj_matrix)
      np.fill_diagonal(exp_adj_matrix,0)

      A = torch.tensor(exp_adj_matrix, dtype=torch.int)


      edge_index = (A > 0).nonzero().t().to(torch.long)

      edge_weights = A[edge_index[0], edge_index[1]].to(torch.float)
      new_d = Data(x=d.x, edge_index=edge_index, edge_weights=edge_weights, y=d.y)
      new_data.append(new_d)

  labels = np.array([data.y.item() for data in new_data])
  train_val_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.2, stratify=labels, random_state=12345,shuffle= True)

  train_val_data = [new_data[i] for i in train_val_idx]
  test_data = [new_data[i] for i in test_idx]
  test_labels = labels[test_idx]

  # Initialize StratifiedKFold for cross-validation on the training+validation set
  # Extract labels for the training+validation set for stratified cross-validation
  train_val_labels = labels[train_val_idx]
  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
  criterion = torch.nn.BCEWithLogitsLoss()
  skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=12345)

  # Initialize lists to store test accuracy
  test_accuracy,val_accuracy = [],[]
  # Loop over folds
  for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(train_val_labels)), train_val_labels)):
      train_subset = [train_val_data[i] for i in train_idx]
      val_subset = [train_val_data[i] for i in val_idx]

      train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
      val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

      model = GCN_weighted(hidden_channels=32,train_data = train_loader, num_features=num_features, num_layers=3, gnn= "GraphConv")
      optimizer = torch.optim.Adam(model.parameters(), lr=lr)

      print(f'Fold {fold+1}:')
      print(f'Number of training graphs: {len(train_subset)}')
      print(f'Number of validation graphs: {len(val_subset)}')

      best_val_acc = 0
      for epoch in range(1, 100):
          loss = train_weights(model = model, loader = train_loader, optimizer = optimizer, criterion=criterion)
          train_acc = test_weights(model, train_loader)
          val_acc = test_weights(model, val_loader)
          test_acc = test_weights(model, test_loader)

          if epoch%30==0:
            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, val Acc: {val_acc:.4f}  test_acc: {test_acc}')
          if val_acc>best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(),'model_with_weights-best-acc.pkl')
            #### save your model in a file : model.save_to_dict

      model.load_state_dict(torch.load('model_with_weights-best-acc.pkl'))
      model.eval()
      test_acc_ = test_weights(model,test_loader)
      test_accuracy.append(test_acc_)
      val_accuracy.append(best_val_acc)


  res = [name, "no",batch_size, lr, num_folds, np.mean(val_acc),np.mean(test_accuracy)]
  print(res)
  writer.writerow(res)
file.close()
print("experiment complete")

from google.colab import drive
drive.mount('/content/drive')